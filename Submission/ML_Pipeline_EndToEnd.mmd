sequenceDiagram
    participant Platform as Platform Adapters<br/>(WhatsApp/iMessage/IMAP)
    participant Extractor as Data Extraction Layer
    participant Normalizer as Schema Normalizer
    participant NLP as spaCy NLP Pipeline<br/>(Tokenization, NER, Lemmatization)
    participant MacApp as Mac App<br/>(Encryption Manager)
    participant S3 as S3 Staging
    participant SQS as SQS Queue
    participant Lambda as AWS Lambda<br/>(Ephemeral Processing)
    participant OpenAI as OpenAI API<br/>text-embedding-3-small
    participant SBERT as Sentence-BERT<br/>(Local Fallback)
    participant PG as PostgreSQL + pgvector<br/>(HNSW Index)
    
    Note over Platform,PG: Stage 1: Platform-Specific Extraction
    
    Platform->>Extractor: Raw message data<br/>(JSON/SQLite/MIME)
    
    alt WhatsApp
        Extractor->>Extractor: Parse message.conversation<br/>Extract external_id, timestamp, participants
    else iMessage
        Extractor->>Extractor: SQL JOIN (message ⋈ handle ⋈ chat)<br/>Parse attributedBody BLOB (Ventura+)<br/>Convert Apple dates: unixTime = (appleTime/1e9) + 978307200
    else Email/Calendar
        Extractor->>Extractor: Recursive MIME parsing<br/>Extract text/plain, text/html<br/>Parse iCalendar .ics VEVENT
    end
    
    Extractor-->>Normalizer: Platform-specific data
    
    Note over Platform,PG: Stage 2: Schema Normalization
    
    Normalizer->>Normalizer: Convert to unified Message schema:<br/>{id, platform, timestamp, sender,<br/>recipients, content, metadata}
    Normalizer->>Normalizer: Resolve contact identities<br/>(fuzzy matching, canonical IDs)
    
    Normalizer-->>NLP: Unified Message object
    
    Note over Platform,PG: Stage 3: Text Preprocessing (NLP)
    
    NLP->>NLP: 1. Tokenization<br/>Split text into words/subwords<br/>Handle emojis, URLs, @mentions
    NLP->>NLP: 2. Language Detection<br/>spaCy langdetect (70+ languages)<br/>Detect: "en", "es", "de", etc.
    NLP->>NLP: 3. Named Entity Recognition (NER)<br/>Extract: PERSON, ORG, GPE, DATE, TIME, MONEY<br/>Accuracy: 85-95%
    NLP->>NLP: 4. Lemmatization<br/>"meetings" → "meeting"<br/>"discussed" → "discuss"
    NLP->>NLP: 5. Stop Word Removal (optional)<br/>Remove: "the", "a", "to", etc.
    
    NLP-->>MacApp: Clean preprocessed text +<br/>embedding_metadata:<br/>{preprocessed_text, detected_language,<br/>named_entities, tokens, lemmas}
    
    Note over Platform,PG: Stage 4: Embedding Generation (Dual Path)
    
    rect rgba(251, 191, 36, 0.2)
        Note over MacApp,OpenAI: Path A: Cloud Processing (Primary - 62.3% MTEB)
        
        MacApp->>MacApp: Generate ephemeral key<br/>(256-bit AES, 5-min TTL)
        MacApp->>MacApp: Encrypt batch (100 messages)<br/>with ephemeral key
        MacApp->>S3: Upload encrypted blobs<br/>/staging/batch-{id}/msg-*.enc
        MacApp->>SQS: Queue job {key_id, ephemeral_key,<br/>s3_keys, ttl_expires_at}
        
        SQS->>Lambda: Trigger Lambda function
        Lambda->>Lambda: Validate key not expired<br/>(check ttl_expires_at)
        Lambda->>S3: Download encrypted messages
        Lambda->>Lambda: Decrypt IN MEMORY<br/>(ephemeral_key, NEVER persisted)
        Lambda->>OpenAI: POST /v1/embeddings<br/>model: text-embedding-3-small<br/>input: [100 preprocessed texts]<br/>encoding_format: float
        OpenAI-->>Lambda: Return embeddings:<br/>100 × 1536-dimensional vectors<br/>[0.234, -0.156, 0.877, ..., 0.445]
        Lambda->>Lambda: Re-encrypt embeddings<br/>with user permanent key
        Lambda->>PG: INSERT encrypted embeddings
        Lambda->>S3: DELETE staging files (cleanup)
        Lambda->>Lambda: Zero out ephemeral_key<br/>Terminate (clear memory)
    end
    
    rect rgba(6, 182, 212, 0.2)
        Note over MacApp,SBERT: Path B: Local Processing (Fallback - 58% MTEB)
        
        alt OpenAI unavailable OR User Privacy Mode
            MacApp->>SBERT: Load model<br/>all-MiniLM-L6-v2 (80 MB)
            SBERT->>SBERT: model.encode(preprocessed_texts,<br/>batch_size=32,<br/>convert_to_tensor=False)
            SBERT-->>MacApp: Return embeddings:<br/>100 × 384-dimensional vectors<br/>(smaller but private)
            MacApp->>MacApp: Encrypt embeddings<br/>with user permanent key
            MacApp->>PG: INSERT encrypted embeddings<br/>(separate column for 384-dim)
        end
    end
    
    Note over Platform,PG: Stage 5: Vector Indexing & Storage
    
    PG->>PG: Store in message_embeddings table:<br/>CREATE TABLE message_embeddings (<br/>  message_id UUID,<br/>  embedding vector(1536),<br/>  embedding_local vector(384),<br/>  user_id UUID<br/>)
    PG->>PG: Build HNSW index:<br/>CREATE INDEX ON message_embeddings<br/>USING hnsw (embedding vector_cosine_ops)<br/>WITH (m=16, ef_construction=64)
    PG->>PG: Index properties:<br/>• O(log n) query complexity<br/>• 95%+ recall@10<br/>• 433x faster than brute force<br/>• 120ms p95 for 100K vectors
    
    PG-->>MacApp: Confirmation: Embeddings indexed<br/>Ready for semantic search

    Note over Platform,PG: Performance Metrics:<br/>• Cloud embedding: 2ms per message (batched)<br/>• Local embedding: 50ms per message (Mac M1)<br/>• NLP preprocessing: 10,000 words/sec<br/>• Vector search: 120ms p95 (100K vectors)<br/>• End-to-end: <5s (extraction → searchable)

    %% Styling for dark background with high contrast
    %%{init: {'theme':'dark', 'themeVariables': { 
        'actorBkg':'#2563eb',
        'actorBorder':'#60a5fa',
        'actorTextColor':'#ffffff',
        'actorLineColor':'#60a5fa',
        'signalColor':'#e0e7ff',
        'signalTextColor':'#e0e7ff',
        'labelBoxBkgColor':'#1e293b',
        'labelBoxBorderColor':'#475569',
        'labelTextColor':'#f1f5f9',
        'loopTextColor':'#fbbf24',
        'altTextColor':'#fb923c',
        'noteBkgColor':'#065f46',
        'noteBorderColor':'#10b981',
        'noteTextColor':'#ffffff',
        'activationBkgColor':'#7c3aed',
        'activationBorderColor':'#a78bfa',
        'sequenceNumberColor':'#ffffff'
    }}}%%

